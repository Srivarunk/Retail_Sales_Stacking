{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImportingRequiredLibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "pd.options.display.max_columns = 1000\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "macro = pd.read_excel('macro_economic.xlsx')\n",
    "weather = pd.read_excel('weather.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2922, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the null values in WeatherEvent column with mode og=f that column\n",
    "weather['WeatherEvent'] = weather['WeatherEvent'].fillna(weather['WeatherEvent'].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows with T is 205\n",
      "It is 7 percent of the entire dataset.\n"
     ]
    }
   ],
   "source": [
    "#Finding the percentage of rows with 'T' in 'precip (mm) sum' column.\n",
    "count=0\n",
    "for i in weather['Precip.\\xa0(mm) sum']=='T':\n",
    "    if i==True:\n",
    "        count=count+1\n",
    "print('Total number of rows with T is', count)\n",
    "print('It is',round((count/len(weather.index))*100), 'percent of the entire dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As they are only 7% of data we can remove those rows.\n",
    "weather['Precip.\\xa0(mm) sum'] = weather['Precip.\\xa0(mm) sum'].replace('T', np.nan)\n",
    "weather = weather.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1826 index column has string values which is inappropriate, hence we can drop it.\n",
    "weather = weather.drop([1826])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows with - is 16\n",
      "It is 1 percent of the entire dataset.\n"
     ]
    }
   ],
   "source": [
    "#Counting the percentage of rows with '-'\n",
    "count=0\n",
    "for i in weather['Sea Level Press.\\xa0(hPa) avg']=='-':\n",
    "    if i==True:\n",
    "        count=count+1\n",
    "print('Total number of rows with - is', count)\n",
    "print('It is',round((count/len(weather.index))*100), 'percent of the entire dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT is only 1 percent of the whole data, hence we can drop these rows.\n",
    "weather = weather.replace('-',np.nan)\n",
    "weather = weather.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding 'WeatherEvent' column for aggregating columns by  year and month\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "weather.WeatherEvent = le.fit_transform(weather.WeatherEvent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating columns by Year and Month\n",
    "weather = weather.groupby(by=['Year','Month'])[weather.columns[3:]].mean()\n",
    "weather = weather.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding months with its respective number.\n",
    "a = {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}\n",
    "weather.Month = weather['Month'].map(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 22)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting year and month by '-' and storing themm as seperate columns and dropping Year-Month column\n",
    "k = macro['Year-Month'].str.split(' - ',expand=True)\n",
    "macro['Year'] = k[0]\n",
    "macro['Month'] = k[1]\n",
    "del macro['Year-Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Mapping months with its respective number as in 'macro' dataset and label encoding months with its respective number as in 'weather' dataset\n",
    "macro.Year = macro.Year.astype(int)\n",
    "\n",
    "a = {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}\n",
    "macro.Month = macro['Month'].map(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing '?' with median of AdvertisingExpenses column.\n",
    "macro['AdvertisingExpenses (in Thousand Dollars)'] = macro['AdvertisingExpenses (in Thousand Dollars)'].replace('?', np.nan)\n",
    "macro['AdvertisingExpenses (in Thousand Dollars)'] = macro['AdvertisingExpenses (in Thousand Dollars)'].fillna(macro['AdvertisingExpenses (in Thousand Dollars)'].median())\n",
    "macro['AdvertisingExpenses (in Thousand Dollars)'] = macro['AdvertisingExpenses (in Thousand Dollars)'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the values in this column are same hence we can drop this column\n",
    "macro = macro.drop('PartyInPower',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 18)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train,Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 'data' columns to both train and test and filling those columns as train and test respectively.\n",
    "train['data'] = 'train'\n",
    "test['data'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows with missing values is  12\n",
      "It is 6 percent of the entire dataset.\n"
     ]
    }
   ],
   "source": [
    "# Counting the percentage of null valued rows in train dataset\n",
    "count=0\n",
    "for i in train.isnull().sum(axis=1):\n",
    "    if i>0:\n",
    "        count=count+1\n",
    "print('Total number of rows with missing values is ', count)\n",
    "print('It is',round((count/len(train.index))*100), 'percent of the entire dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null valued rows in train as they are only 6% of total dataset.\n",
    "train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concating train and test datasets\n",
    "train_test = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging macro and train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging weather and macro datasets\n",
    "final = train_test.merge(weather,on=['Year','Month'],how='inner').merge(macro,on=['Year','Month'],\n",
    "                                                                        how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segregrating Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final[final['data']=='train']\n",
    "test = final[final['data']=='test']\n",
    "\n",
    "\n",
    "test.drop(['data','Sales(In ThousandDollars)'],axis =1,inplace =True)\n",
    "train.drop('data',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting full train into X(independent variables) and Y(dependent variables)\n",
    "X = train.drop('Sales(In ThousandDollars)',axis =1)\n",
    "y = train[['Sales(In ThousandDollars)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding ProductCategory column in train\n",
    "dummies_train = pd.get_dummies(X.ProductCategory,drop_first=True)\n",
    "X = pd.concat([X,dummies_train],axis = 1)\n",
    "X.drop('ProductCategory',axis = 1,inplace = True)\n",
    "\n",
    "#One Hot encoding ProductCategory column in test\n",
    "dummies_test = pd.get_dummies(test.ProductCategory,drop_first=True)\n",
    "test = pd.concat([test,dummies_test],axis = 1)\n",
    "test.drop('ProductCategory',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xval,ytrain,yval = train_test_split(X,y,test_size = 0.3,random_state=1)\n",
    "\n",
    "# Scaling \n",
    "sc = StandardScaler()\n",
    "\n",
    "xtrain_scaled = sc.fit_transform(xtrain)\n",
    "xval_scaled = sc.transform(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "RMSE:  120.00722927069874\n",
      "R2 value:  99.14353386810338\n",
      "TEST\n",
      "RMSE:  255.48893314310538\n",
      "R2 value:  94.4165053038367\n"
     ]
    }
   ],
   "source": [
    "#Applying Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=300)\n",
    "rf.fit(xtrain,ytrain)\n",
    "ypred = rf.predict(xtrain)\n",
    "rf_ypred = rf.predict(xval)\n",
    "print('TRAIN')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(ytrain,ypred)))\n",
    "print('R2 value: ',r2_score(ytrain,ypred)*100)\n",
    "print('TEST')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(yval,rf_ypred)))\n",
    "print('R2 value: ',r2_score(yval,rf_ypred)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf.fit(X,y)\n",
    "# csv = pd.DataFrame({'S.No.':np.arange(1,37),'Sales':rf.predict(test)})\n",
    "# csv.to_csv('7_Submission_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "RMSE:  332.5092039709577\n",
      "R2 value:  93.42489460605212\n",
      "TEST\n",
      "RMSE:  332.85949630756687\n",
      "R2 value:  90.5227175151274\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':[100,300,500],\n",
    "          'max_features':[20,30,40],\n",
    "          'min_samples_leaf':[10,15,20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf,param_grid=params,cv = 5)\n",
    "grid_search.fit(xtrain,ytrain)\n",
    "grid_search.best_params_\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(xtrain,ytrain)\n",
    "ypred = best_rf.predict(xtrain)\n",
    "best_rf_ypred = best_rf.predict(xval)\n",
    "print('TRAIN')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(ytrain,ypred)))\n",
    "print('R2 value: ',r2_score(ytrain,ypred)*100)\n",
    "print('TEST')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(yval,best_rf_ypred)))\n",
    "print('R2 value: ',r2_score(yval,best_rf_ypred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_rf.fit(X,y)\n",
    "# csv = pd.DataFrame({'S.No.':np.arange(1,37),'Sales':best_rf.predict(test)})\n",
    "# csv.to_csv('6_Submission_best_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "RMSE:  193.1961683717743\n",
      "R2 value:  97.78030965027176\n",
      "TEST\n",
      "RMSE:  298.3425068881091\n",
      "R2 value:  92.38636256551625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ad = AdaBoostRegressor()\n",
    "\n",
    "ad.fit(xtrain,ytrain)\n",
    "ypred = ad.predict(xtrain)\n",
    "ad_ypred = ad.predict(xval)\n",
    "\n",
    "print('TRAIN')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(ytrain,ypred)))\n",
    "print('R2 value: ',r2_score(ytrain,ypred)*100)\n",
    "print('TEST')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(yval,ad_ypred)))\n",
    "print('R2 value: ',r2_score(yval,ad_ypred)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "RMSE:  179.45913657781458\n",
      "R2 value:  98.08474535622\n",
      "TEST\n",
      "RMSE:  252.1952413625487\n",
      "R2 value:  94.55953904698679\n"
     ]
    }
   ],
   "source": [
    "params = {'learning_rate':[1.0,1.1,1.3,1.6,0.1], \n",
    "          'loss':['linear'],\n",
    "          'n_estimators':[200,400,600,700,900]\n",
    "         }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(ad,param_grid=params,cv = 5)\n",
    "\n",
    "grid_search.fit(xtrain,ytrain)\n",
    "\n",
    "best_ad = grid_search.best_estimator_\n",
    "\n",
    "best_ad.fit(xtrain,ytrain)\n",
    "ypred = best_ad.predict(xtrain)\n",
    "best_ad_ypred = best_ad.predict(xval)\n",
    "\n",
    "print('TRAIN')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(ytrain,ypred)))\n",
    "print('R2 value: ',r2_score(ytrain,ypred)*100)\n",
    "print('TEST')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(yval,best_ad_ypred)))\n",
    "print('R2 value: ',r2_score(yval,best_ad_ypred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_ad.fit(X,y)\n",
    "# csv = pd.DataFrame({'S.No.':np.arange(1,37),'Sales':best_ad.predict(test)})\n",
    "# csv.to_csv('1_Submission_best_ad.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Regressor with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "RMSE:  115.80009600286306\n",
      "R2 value:  99.20253209198484\n",
      "TEST\n",
      "RMSE:  248.62133616705353\n",
      "R2 value:  94.71264202779189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "br = BaggingRegressor()\n",
    "\n",
    "params = ({'n_estimators':[300,400,600],\n",
    "    'bootstrap':[True,False],\n",
    "    'bootstrap_features':[True,False]})\n",
    "\n",
    "grid_search = GridSearchCV(br,param_grid=params,cv = 5)\n",
    "\n",
    "grid_search.fit(xtrain,ytrain)\n",
    "\n",
    "best_br = grid_search.best_estimator_\n",
    "\n",
    "best_br.fit(xtrain,ytrain)\n",
    "ypred = best_br.predict(xtrain)\n",
    "best_br_ypred = best_br.predict(xval)\n",
    "\n",
    "print('TRAIN')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(ytrain,ypred)))\n",
    "print('R2 value: ',r2_score(ytrain,ypred)*100)\n",
    "print('TEST')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(yval,best_br_ypred)))\n",
    "print('R2 value: ',r2_score(yval,best_br_ypred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #best_br.fit(X,y)\n",
    "# csv = pd.DataFrame({'S.No.':np.arange(1,37),'Sales':best_br.predict(test)})\n",
    "# csv.to_csv('3_Submission_best_br.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "RMSE:  11.81417651569325\n",
      "R2 value:  99.99169953759443\n",
      "TEST\n",
      "RMSE:  142.9811584503067\n",
      "R2 value:  98.25128294687597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor(n_estimators=300,learning_rate=0.1)\n",
    "\n",
    "gb.fit(xtrain,ytrain)\n",
    "ypred = gb.predict(xtrain)\n",
    "gb_ypred = gb.predict(xval)\n",
    "\n",
    "print('TRAIN')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(ytrain,ypred)))\n",
    "print('R2 value: ',r2_score(ytrain,ypred)*100)\n",
    "print('TEST')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(yval,gb_ypred)))\n",
    "print('R2 value: ',r2_score(yval,gb_ypred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb.fit(X,y)\n",
    "# csv = pd.DataFrame({'S.No.':np.arange(1,37),'Sales':gb.predict(test)})\n",
    "# csv.to_csv('2_Submission_gb.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "RMSE:  8.346977398652168\n",
      "R2 value:  99.99585662872104\n",
      "TEST\n",
      "RMSE:  144.61012412678\n",
      "R2 value:  98.21121015752692\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': [100,200,300,400], 'learning_rate': [0.1, 0.5, 1.],\n",
    "           'subsample': [0.05,1.0],\n",
    "           'max_features': [0.05,1.0] }\n",
    "\n",
    "grid_search = GridSearchCV(gb,param_grid=params,cv=5)\n",
    "\n",
    "grid_search.fit(xtrain,ytrain)\n",
    "\n",
    "best_gb = grid_search.best_estimator_\n",
    "\n",
    "best_gb.fit(xtrain,ytrain)\n",
    "ypred = best_gb.predict(xtrain)\n",
    "best_gb_ypred = best_gb.predict(xval)\n",
    "\n",
    "print('TRAIN')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(ytrain,ypred)))\n",
    "print('R2 value: ',r2_score(ytrain,ypred)*100)\n",
    "print('TEST')\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(yval,best_gb_ypred)))\n",
    "print('R2 value: ',r2_score(yval,best_gb_ypred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_gb.fit(X,y)\n",
    "# csv = pd.DataFrame({'S.No.':np.arange(1,37),'Sales':best_gb.predict(test)})\n",
    "# csv.to_csv('4_Submission_best_gb.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.9988143431250104\n"
     ]
    }
   ],
   "source": [
    "#Lets stack RandomForest,AdaBoost,BaggingRegressor,GradientBooster\n",
    "df = pd.DataFrame({'RF':rf.predict(X),'Ada':best_ad.predict(X),'BR':best_br.predict(X),'GB':best_gb.predict(X)})\n",
    "\n",
    "stacked_model=RandomForestRegressor(n_estimators=300)\n",
    "stacked_model.fit(df,y)\n",
    "ypred = stacked_model.predict(df)\n",
    "print('Train R2:', r2_score(y,ypred))\n",
    "\n",
    "#Test\n",
    "df_test = pd.DataFrame({'RF':rf.predict(test),'Ada':best_ad.predict(test),'BR':best_br.predict(test),'GB':best_gb.predict(test)})\n",
    "test_ypred = stacked_model.predict(df_test)\n",
    "csv = pd.DataFrame({'S.No.':np.arange(1,37),'Sales':test_ypred})\n",
    "#csv.to_csv('8_Submission_stack.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
